\section{Expérience D3} \label{expD3}
  \subsection{Objectif}
    En partant de la seconde architecture de \cite{Cleeremans_2007}, 
    comprendre de quelles manières un réseau de neurone connexionniste peut, à partir de ses propres paris
    sur son résultat, améliorer son comportement.
  
  
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
      par 256 (16x16) neurones d'entrées. Il est composé d'une couche cachée de 100 neurones.
      
      Un second réseau de perceptron multicouche apprend à parier sur la qualité de la réponse
      du premier réseau à partir de sa couche cachée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.
      
      Lorsque le second réseau parie bas, le taux d'apprentissage du premier réseau est augmenté
      et le momentum est diminué. À l'inverse, lorsqu'il parie haut, c'est que la forme est déjà
      apprise, le taux d'apprentissage sera faible et le momentum élevé.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=250px]{data/expD1/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item momentum : 0. sur le second réseau
	      \item taux d'apprentissage : 0.15 sur les second réseau
	      \item \textbf{1600 formes} de chiffres différents présentées (shuffle) \cite{Handwritten_256}
	      \item apprentissage 50 (formes) x 300 (époques)
	      \item utilisation de biais
	      
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item poids initialisés sur [-1 ; 1] pour le premier réseau
	      \item poids initialisés sur [-0.25 ; 0.25] pour le second réseau
	      \item entrées valent 0 ou 1
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\includegraphics[width=250px]{data/expD3/perff.png}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item la performance de classification représente le taux de bonnes réponses (winner-take-all) pour les 50 formes présentées sur une époque
	  \item momentum et learning rate sont la moyenne des paramètres appliquées au premier réseau sur une époque
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item On obtient une hausse de performance
	  \item Comme la règle le défini, à la fin (ie. paris hauts) le momentum est élevé et le taux d'apprentissage faible
	\end{itemize}
    \paragraph{Secondaires}
      Analyse des performances sous-jacentes
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expD3/rms.png}
	  &
	  \includegraphics[width=250px]{data/expD3/perf.png} 
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item la courbe rouge représentent le taux de paris hauts du second réseau
	  \item la performance de classification représente le taux de bonnes réponses (winner-take-all) pour les 50 formes présentées sur une époque
	  \item formule utilisée pour RMS (cf. Formules~\nameref{rms})
	\end{itemize}
      \subparagraph{Conclusion}
	Il faut se méfier du RMS, car même si l'erreur globale sur un neurone est plus faible avec control, 
	durant la classification on cherche juste le neurone le plus actif.


  \subsection{Conclusion}
    L'expérience est plutôt concluente, le bémol vient du fait que c'est l'humain qui règle le comportement du réseau : le taux d'apprentissage/momentum
    à utiliser dans tel ou tel cas.
    
    Dans l'\nameref{expD4}, nous allons tenter de laisser le réseau le faire lui même.
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    
    \input{gradient}
    
\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
