\section{Introduction
\label{sec:intro}}
% ------------------------------------------------------------
\subsection{SoN apprend plus vite
\label{sec:app_vite}}
En regardant les résultats des expériences \nameref{expA1}, \nameref{expA2} et \nameref{expA3}, on peut s'interroger sur l'expression ``apprendre plus vite''.

Si on regarde l'erreur de classification (graphes de droite) on voit~:
\begin{itemize}
\item expA1~: au début, SoN classifie mieux mais à partir de l'épisode 180, ce n'est plus le cas.
\item expA2~: FoN classifie toujours mieux que SoN.
\item expA3~: il semble que FoN classifie toujours un peu mieux que SoN.
\end{itemize}

Par contre, il est vrai que les courbes d'erreur normalisées peuvent faire penser que SoN apprend plus vite. Les graphes montrent que SoN apprend rapidement à ``recopier'' la couche cachée de FoN (mais c'est une input). 
Par contre il apprend beaucoup moins facilement à retrouver les entrées, surtout dans le cas des données manuscrites (ce qui paraît logique, car on n'est plus dans des relations bijectives entre les couches d'entrée et cachées de FoN). 
Pour ce qui est d'apprendre les valeurs de la couche de sortie, on est assez surpris de constater que l'apprentissage est plus rapide que pour la couche cachée.

Au final, on peut se demander ce qu'on entend par ``apprendre plus vite'' pour SoN.
% ------------------------------------------------------------
\subsection{Expériences B
\label{sec:expB}}

Des représentations pertinentes pour différencier les différentes classes en entrées semblent effectivement apprises relativement vite car, quand on fige les poids de la couche cachée de FoN, 
le changement de tâche (qui sont toujours des permutations sur la couche de sortie) est appris rapidement. 
Quand on ne fige pas les poids, j'imagine que la couche cachée essaie de se reconfigurer, ce qui est plus lent, et sans doute d'autant plus difficile 
que certains neurones doivent saturer et que leur dérivée est donc quasi-nulle, ce qui ralenti le changement de valeur des poids.

SoN apprend lentement pour la même raison : 
la couche cachée essaie de se reconfigurer, ce qui ne doit pas être rapide pour au moins quelques un de ses neurones (ceux qui saturent)... Pour preuve l'\nameref{expB4} montre que SoN arrive à se reconfigurer
quand ses propres poids sont bloqués.

Si on prend des tâches qui ne sont plus de simples permutations, ça semble marcher quand même (cf \nameref{expB5} ).

% ------------------------------------------------------------
\subsection{Expériences C
\label{sec:expC}}

Deux questions ont motivé la reprise de cette expérience et le fait de jouer un peu avec les paramètres des réseaux. 

D'une part, nous voulions en savoir plus sur le comportement du réseau SoN. Quand on a vu qu'il se ``contentait'' de parier ``low'' pendant les premiers épisodes (tant que FoN fait plus de 50\% d'erreur) 
puis ``high'' ensuite (quand le taux de succès de FoN dépasse 50\%), nous nous sommes dit que c'était le plus comportement le plus simple. Mais pas forcément le plus efficace.
Ainsi, dans un deuxième temps, comme nous étions persuadés qu'avec 100 neurones dans la couche cachée, le SoN devrait être capable d'apprendre ``par coeur'' les cas où FoN se trompe, nous avons
expérimenté avec les paramètres d'apprentissage jusqu'à trouver les résultats présentés qui confirme ce que nous pensions.

L'\nameref{expC3} ne fonctionne pas car les données de la couche cachée ne discriment pas de façon non ambigüe les entrées. Il y a sans doute une généralisation effectuée par la couche cachée du FoN.

% ------------------------------------------------------------
\subsection{Expériences D
\label{sec:expD}}

Les \nameref{expD3} et \nameref{expD4} sont intéressantes, elles montrent qu'il est possible pour une architecture de ``monitorer'' son comportement de manière à influencer les parmètres d'apprentissage 
pour augmenter les performances.
