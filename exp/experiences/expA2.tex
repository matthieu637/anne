\section{Expérience A2} \label{expA2}
  \subsection{Objectif}
    Comprendre de quelles manières peuvent émerger des représentations et méta-représentations dans 
    un réseau de neurone connexionniste, plus particulièrement sur des perceptrons multicouches.
    
    Dans l'optique d'éprouver la première expérience de l'article \cite{Cleeremans_2007} sur des
    données plus réelles, il sagit, dans un premier temps, de montrer qu'une augmentation du 
    nombre de neurones, et qu'un simple agrandissement des chiffres en entrées, 
    n'affecte pas fondamentalement le fonctionnement du réseau.

  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
      par 256 (16x16) neurones d'entrées. Il est composé d'une couche cachée de 64 neurones.
      
      Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
      réseau en n'ayant que sa couche cachée en entrée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=220px]{data/expA2/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{220px}
	    \begin{itemize}
	      \item momentum : 0.9 sur les 2 réseaux
	      \item taux d'apprentissage : 0.1 sur les 2 réseaux
	      \item \textbf{10 formes} de chiffres différents présentées (shuffle)
	      \item apprentissage 10 (formes) x 1000 (époques)
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{205px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25]
	      \item taux d'apprentissage constant
	      \item entrées valent 0 ou 1
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA2/rms.png}
	  &
	  \includegraphics[width=250px]{data/expA2/err.png} 
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item formule utilisée pour RMS (cf. Formules~\nameref{rms})
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) sur les couches à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item le premier réseau réussit à apprendre sa tâche de classification
	  \item la couche cachée et la couche de sortie ne posent aucun problèmes d'apprentissage
	  \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	  \item le second réseau apprend plus rapidement que le premier
	  \item l'augmentation du nombre de neurone ne change pas les tendances des courbes
	\end{itemize}
    \paragraph{Secondaires}
      Discrétisation de la couche cachée du premier réseau
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA2/discretize_cloud.png}
	  &
	  \includegraphics[width=250px]{data/expA2/discretize.png} 
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item une couleur équivaut à un chiffre présenté
	  \item une valeur discretisée correspond à un certain encodage de la couche cachée (cf Algorithmes~\nameref{discretize})
	\end{itemize}
      \subparagraph{Conclusion}
	Les neurones se stabilisent très rapidement (autour de la 50\up{ième} époque en moyenne), 
	le tout permettant au second réseau d'avoir des entrées très peu variables, favorisant
	son apprentissage.
    \paragraph{Secondaires}
      Représentations au travers des poids du premier réseau
      \begin{center}
	Couche cachée \\
	\includegraphics[width=250px]{data/expA2/representation_hidden.png}
      \end{center}
      \begin{center}
	Couche de sortie \\
	\includegraphics[width=250px]{data/expA2/representation.png}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item plus une case est noire, plus sa présence est importante pour le chiffre en question
	  \item plus une case est blanche, plus son absence est importante
	\end{itemize}
      \subparagraph{Conclusion}
      Il est intéréssant de remarquer qu'au vu du peu d'entrée, le réseau cible des neurones très précis.
      Pour le chiffre 2, par exemple, seul 3 lignes sont ciblées.
    \paragraph{Secondaires}
      Prototypes à l'intérieur de la première partie de la couche de sortie du second réseau
      \begin{center}
	\includegraphics[width=250px]{data/expA2/prototype.png}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item Il sagit de la moyenne des réponses du second réseaux sur toutes les entrées
	\end{itemize}
      \subparagraph{Conclusion}
	Le peu d'entrées permet l'apprentissage par-coeur de chaque forme.
	


  \subsection{Conclusion}
    L'augmentation du nombre de neurones dans le réseau, et l'augmentation de la taille des entrées
    ne modifient pas l'attitude du réseau.
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    \input{discretize}
    \input{gradient}

\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
