\section{Expérience C4}
  \subsection{Objectif}
    Reproduction et approfondissement des résultats de la première expérience 1 dans l'article 
    \cite{Cleeremans_2007}. 

  
  
    Comprendre de quelles manières peuvent émerger des représentations et méta-représentations dans 
    un réseau de neurone connexionniste, en particulier sur des perceptrons multicouches.
  
  
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
      par 20 neurones d'entrées. Il est composé d'une couche cachée de 5 neurones.
      
      Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
      réseau en n'ayant que sa couche cachée en entrée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=220px]{data/expC4/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{220px}
	    \begin{itemize}
	      \item momentum : 0.9 sur les 2 réseau
	      \item taux d'apprentissage : 0.1 sur les 2 réseau
	      \item 10 chiffres différents présentés
	      \item apprentissage 10 (formes) x 1000 (époques)
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{205px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25]
	      \item taux d'apprentissage constant
	      \item entrées valent 0 ou 1
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expC4/rms.png}
	  &
	  \includegraphics[width=250px]{data/expC4/perf.png} 
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) des couches du premier à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item la couche cachée et la couche de sortie ne posent aucun problèmes d'apprentissage
	  \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	  \item le second réseau apprend plus rapidement que le premier
	\end{itemize}

  \subsection{Conclusion}
  
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    
    \input{gradient}
    
\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
