\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc} %commentaire
\usepackage[francais]{babel} %FR
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}

\usepackage[pdftex]{graphicx} % img
\usepackage{wrapfig}
\usepackage{float}

\usepackage{algpseudocode}

\usepackage[top=2cm, bottom=2cm, left=2.5cm, right=2.5cm]{geometry} %Réduire les marges


% Style Page
\pagestyle{fancy} % entêtes
% \fancyhf{}

\setlength{\headheight}{15pt}
\renewcommand{\sectionmark}[1]{\markright{#1}} 
\lhead{ \rightmark }

\sloppy % ne pas faire déborder les lignes dans la marge


\begin{document}
  \section*{Expérience 1} \sectionmark{EXPÉRIENCE 1}
    \subsection*{Résumé}
      Reproduction et approfondissement des résultats de la première expérience 1 dans l'article 
      \cite{Cleeremans_2007}. 

    
    \subsection*{But}
      Comprendre de quelles manières peuvent émerger des représentations et méta-représentations dans 
      un réseau de neurone connexionniste, en particulier sur des perceptrons multicouches.
    
    
    \subsection*{Architecture}
      \paragraph*{Description}
	Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
	par 20 neurones d'entrées. Il est composé d'une couche cachée de 5 neurones.
	
	Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
	réseau en n'ayant que sa couche cachée en entrée.

      \paragraph*{Schéma}
	\begin{center}
	  \includegraphics[width=200px]{../cleeremans_2007/digit_reco/digit_reco.png}
	\end{center}
	
      \paragraph*{Paramètres}
        \begin{center}
	  \begin{tabular}{lr}
	    \begin{minipage}{220px}
	      \begin{itemize}
		\item momentum : 0.9 sur les 2 réseau
		\item taux d'apprentissage : 0.1 sur les 2 réseau
		\item 10 chiffres différents présentés
		\item apprentissage 10 (formes) x 1000 (époques)
		\item utilisation de biais
	      \end{itemize}
	    \end{minipage}
	    &
	    \begin{minipage}{205px}
	      \begin{itemize}
		\item poids initialisés sur [-0.25 ; 0.25]
		\item taux d'apprentissage constant
		\item entrées valent 0 ou 1
		\item sigmoïde à température 1
	      \end{itemize}
	    \end{minipage}
	  \end{tabular}
	\end{center}

    
    \newpage
    \subsection*{Résultats}
      \paragraph*{Principaux}
	Analyse des performances
	\begin{center}
	  \begin{tabular}{lr}
	    \hspace*{-1cm}
	    \includegraphics[width=250px]{../cleeremans_2007/digit_reco/rms_ffa.png}
	    &
	    \includegraphics[width=250px]{../cleeremans_2007/digit_reco/err_ffa.png} 
	  \end{tabular}
	\end{center}
	\subparagraph*{Notes}
	  \begin{itemize}
	    \item la courbe violette est la somme des 3 courbes des couches à reproduire.
	    \item 0.3 est le seuil de tolérance pour l'erreur sur un neurone de la couche cachée
	  \end{itemize}
	\subparagraph*{Conclusion}
	  \begin{itemize}
	    \item la couche cachée et la couche de sortie ne posent aucun problèmes d'apprentissage
	    \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	    \item le second réseau apprend plus rapidement que le premier
	  \end{itemize}
      \paragraph*{Secondaires}
	Discrétisation de la couche cachée du premier réseau
      	\begin{center}
	  \begin{tabular}{lr}
	    \hspace*{-1cm}
	    \includegraphics[width=250px]{../cleeremans_2007/digit_reco/discretize_cloud.png}
	    &
	    \hspace*{-1cm}
	    \includegraphics[width=290px]{../cleeremans_2007/digit_reco/discretize.png} 
	  \end{tabular}
	\end{center} 
	\subparagraph*{Notes}
	  \begin{itemize}
	    \item une couleur équivaut à un chiffre présenté
	    \item une valeur discretisée correspond à un certain encodage de la couche cachée (cf Algorithmes)
	  \end{itemize}
	\subparagraph*{Conclusion}
	  Les neurones se stabilisent très rapidement (autour de la 50\up{ième} époque en moyenne), 
	  le tout permettant au second réseau d'avoir des entrées très peu variables, favorisant
	  son apprentissage.
	  


    \subsection*{Conclusion}
    
    

    \newpage 
    \sectionmark{EXPÉRIENCE 1}
    \subsection*{Formules}
      \paragraph{RMS}
	Pour une époque $e$ :
	\begin{center}
	  \begin{large}
	  $ rms\ proportion_{e} = \frac{ rms_{e} = \sqrt{ \frac{1}{n} \sum \limits_{i=1}^{n} 
	  ( o_{i,e} - d_{i} )^2 }}{max(rms_{e'}),\ \forall e' \in epochs } $
	  \end{large}
	$ with \left\lbrace \begin{array}{lll} n : number\ of\ neurons\ on\ the\ output\ 
	layer\\o_{i,e} : value\ obtained\ for\ the\ i^{th}\ neuron\ at\ the\ e^{th}\ epoch\\d_{i} : 
	value\ desired \ for\ the\ i^{th}\ neuron\end{array} \right.$
	\end{center}
      \paragraph{Discrétisation} Pour la couche cachée $hiddenNeuron$ de $n$ neurones, un neurone
	pouvant être encodé par $number\_cutting$ valeurs différentes :
	\begin{center}
	 $\sum \limits_{i=0}^{n} number\_cutting^{i} \times cutting(hiddenNeuron[i]) $
	\end{center}
	\subparagraph{Exemple}
	  $400 \gets [0 ; 0,25 ]\ [0 ; 0,25 ]\  [0,25 ; 0,5 ]\  [0,5 ; 0,75 ]\  [0,25 ; 0,5 ]$ \\
	  \hspace*{2.70cm}
	  $400 \gets 0\times4^0 +   0\times4^1  +   1\times4^2   +  2\times4^3   +   1 \times4^4$
      \paragraph{Descente de gradient} \cite{Touzet_1992} \\
	Construction de l'erreur : 
	  \begin{center}
	    $y_{i} = f'(a_i) \times ( d_i - x_i ) \ si\ i\ neurone\ de\ sortie $ \\
	    $y_{i} = f'(a_i) \times \sum \limits_{k} ( w_{ki} \times y_k )\ si\ i\ neurone\ cache $
	  \end{center}
	Mise à jour des poids :
	  \begin{center}
	    $w_{ij}(t+1) = w_{ij}(t) + learning\_rate \times y_{i} \times x_j + momentum \times 
	    (w_{ij}(t) - w_{ij}(t-1) )$
	  \end{center}
	Variables : 
	  \begin{center}
	    $\left\lbrace \begin{array}{lll} 
	      f : fonction\ sigmoide \\
	      x_i : valeur\ du\ neurone\ i\\
	      d_i : valeur\ desire pour\ le\ neurone\ i\\
	      a_i : somme\ pondere\ des\ poids\ du\ neurone\ i
	    \end{array} \right.$
	  \end{center}
	
  \bibliographystyle{../pre-rapport/apalike}
  \bibliography{../pre-rapport/biblio}

\end{document}



