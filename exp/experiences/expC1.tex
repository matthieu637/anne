\section{Expérience C1} 
  \subsection{Objectif}
    Comprendre de quelles manières un réseau de neurone connexionniste peut parier sur ses propres résultats
    à partir de ses représentations personnelles.
  
  
      Reproduction et approfondissement des résultats de la seconde expérience de l'article 
    \cite{Cleeremans_2007}. 
  
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
      par 20 neurones d'entrées. Il est composé d'une couche cachée de 20 ou 100 neurones.
      
      Un second réseau de perceptron multicouche apprend à parier sur la qualité de la réponse
      du premier réseau à partir de sa couche cachée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=220px]{data/expC1/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{220px}
	    \begin{itemize}
	      \item momentum : 0.5 sur les 2 réseau
	      \item taux d'apprentissage : 0.15 sur les 2 réseau
	      \item 10 chiffres différents présentés
	      \item apprentissage 10 (formes) x 300 (époques)
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{205px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25] pour le second réseau
	      \item poids initialisés sur [-1 ; 1] pour le premier réseau
	      \item taux d'apprentissage constant
	      \item entrées valent 0 ou 1
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expC1/perf_20.png}
	  &
	  \includegraphics[width=250px]{data/expC1/perf_100.png} 
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) des couches du premier à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item la couche cachée et la couche de sortie ne posent aucun problèmes d'apprentissage
	  \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	  \item le second réseau apprend plus rapidement que le premier
	\end{itemize}
    \paragraph{Secondaires}
      RMS
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expC1/rms_20.png}
	  &
	  \includegraphics[width=250px]{data/expC1/rms_100.png} 
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item une couleur équivaut à un chiffre présenté
	  \item une valeur discretisée correspond à un certain encodage de la couche cachée (cf Algorithmes)
	\end{itemize}
      \subparagraph{Conclusion}
	Les neurones se stabilisent très rapidement (autour de la 50\up{ième} époque en moyenne), 
	le tout permettant au second réseau d'avoir des entrées très peu variables, favorisant
	son apprentissage.


  \subsection{Conclusion}
  
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    
    \input{gradient}
    
\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
