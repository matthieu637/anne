\section{Expérience A4} \label{expA4}
  \subsection{Objectif}
    Comprendre de quelles manières peuvent émerger des représentations et méta-représentations dans 
    un réseau de neurone connexionniste, plus particulièrement sur des perceptrons multicouches.
    
    Réalisation de la première expérience de l'article \cite{Cleeremans_2007} sur des données réelles
    et non linéairement séparables.

  
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des fleurs caractérisées
      par 4 neurones d'entrées qui représentent taille et largeur de la pétale et la sépale. 
      Il est composé d'une couche cachée de 5 neurones.
      
      Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
      réseau en n'ayant que sa couche cachée en entrée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=230px]{data/expA4/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item momentum : 0.9 sur les 2 réseaux
	      \item taux d'apprentissage : 0.1 sur les 2 réseaux
	      \item 150 fleurs différentes présentés \cite{Iris}
	      \item apprentissage 10 (formes) x 1000 (époques)
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25]
	      \item taux d'apprentissage constant
	      \item entrées réelles sur [0 ; 1]
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA4/rms.png}
	  &
	  \includegraphics[width=250px]{data/expA4/err.png} 
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item formule utilisée pour RMS (cf. Formules~\nameref{rms})
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) sur les couches à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	  \item l'erreur de classification représente le taux de mauvaises réponses pour les 10 formes présentées sur une époque
	  \item pour SoN input layer et hidden layer, un winner-take-all n'est pas possible (ce n'est pas de la classification, 
	  mais une duplication), il y a donc un seuil d'erreur qui ne doit pas être dépassé par un seul neurone
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item le premier réseau réussit à apprendre sa tâche de classification
	  \item le second réseau réussit à apprendre sa tâche de duplication
	  \item la couche cachée et la couche de sortie posent peu de problèmes d'apprentissage
	  \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	  \item le second réseau apprend plus rapidement que le premier
	\end{itemize}
    \paragraph{Secondaires}
      Discrétisation de la couche cachée du premier réseau
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA4/discretize_cloud.png}
	  &
	  \includegraphics[width=250px]{data/expA4/discretize.png} 
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item une couleur équivaut à une fleur présentée
	  \item une valeur discretisée correspond à un certain encodage de la couche cachée (cf Algorithmes~\nameref{discretize})
	\end{itemize}
      \subparagraph{Conclusion}
	Les données n'étant pas linéairement séparables, leur passage dans une seule couche rend la tâche d'apprentissage
	du second réseau plus complexe.
	
	On peut voir plusieurs points de différentes couleurs aux mêmes endroits (ie. pour la même couche cachée).

  \subsection{Conclusion}
  Le passage sur des données non linéairement séparables se passent bien.
  
  Comme il l'est dit dans \cite{Cleeremans_2007}, cette architecture tente d'abolir les problèmes des réseaux connexionnistes
  classiques à avoir un semblant de conscience.
  
  À savoir :
  \begin{itemize}
   \item qu'ils ne savent pas qu'ils peuvent se trouver dans différents états, et qu'ils ne traitent pas leurs propres états : 
   d'où la présence de ce second réseau qui tente de traiter ses états et d'apprendre qu'il en a plusieurs
   \item des représentations rentant bloquées dans la chaîne de causalité de la tâche à apprendre : d'où
   le fait que ce second réseau n'affecte pas l'apprentissage du premier (ie. pour ne pas retomber dans la chaîne de causalité)
   \\[0.2cm]
  \end{itemize}
  
  Dans l'\nameref{expB3}, nous avons expérimenté les représentations et le transfert de tâche.
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    \input{discretize}
    \input{gradient}

\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
