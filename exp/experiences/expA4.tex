\section{Expérience A4} 
  \subsection{Objectif}
    Comprendre de quelles manières peuvent émerger des représentations et méta-représentations dans 
    un réseau de neurone connexionniste, plus particulièrement sur des perceptrons multicouches.
    
    Réalisation de la première expérience de l'article \cite{Cleeremans_2007} sur des données réelles
    et non linéairement sépérables.

  
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des fleurs caractérisées
      par 4 neurones d'entrées qui représentent taille et largeur de la pétale et la sépale. 
      Il est composé d'une couche cachée de 5 neurones.
      
      Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
      réseau en n'ayant que sa couche cachée en entrée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.

    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=230px]{data/expA4/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item momentum : 0.9 sur les 2 réseaux
	      \item taux d'apprentissage : 0.1 sur les 2 réseaux
	      \item 150 fleurs différentes présentés \cite{Iris}
	      \item apprentissage 10 (formes) x 1000 (époques)
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25]
	      \item taux d'apprentissage constant
	      \item entrées réelles sur [0 ; 1]
	      \item sigmoïde à température 1
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA4/rms.png}
	  &
	  \includegraphics[width=250px]{data/expA4/err.png} 
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item formule utilisée pour RMS (cf. Formules~\nameref{rms})
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) sur les couches à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item le premier réseau réussit à apprendre sa tâche de classification
	  \item la couche cachée et la couche de sortie posent peu de problèmes d'apprentissage
	  \item les performances du second réseau dépendent principalement de sa capacité à reproduire les entrées
	  \item le second réseau apprend plus rapidement que le premier
	\end{itemize}
    \paragraph{Secondaires}
      Discrétisation de la couche cachée du premier réseau
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expA4/discretize_cloud.png}
	  &
	  \includegraphics[width=250px]{data/expA4/discretize.png} 
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item une couleur équivaut à une fleur présentée
	  \item une valeur discretisée correspond à un certain encodage de la couche cachée (cf Algorithmes~\nameref{discretize})
	\end{itemize}
      \subparagraph{Conclusion}
	Les données n'étant pas linéairement sépérables, leur passage dans une seule couche rend la tâche d'apprentissage
	du second réseau plus complexe.
	
	On peut voir plusieurs points de différentes couleurs aux mêmes endroits.
      \subparagraph{Conclusion}
	Le peu d'entrées permet l'apprentissage par-coeur de chaque forme.
	


  \subsection{Conclusion}
  Le passage sur des données non linéairement sépérables se passent très bien.
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    \input{discretize}
    \input{gradient}

\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
