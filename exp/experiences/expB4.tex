\section{Expérience B4} \label{expB4}
  \subsection{Objectif}
  Il sagit d'une extension de l'\nameref{expB2} pour montrer qu'avec des changements de tâche :
  le second réseau est lui aussi plus enclin à apprendre lorsque ses poids sont figés.
   
  \subsection{Architecture}
    \paragraph{Description}
      Un premier réseau de perceptron multicouche apprend à discrétiser des chiffres représentés
      par (16x16) neurones d'entrées. Il est composé d'une couche cachée de 64 neurones.
      
      Un second réseau de perceptron multicouche apprend à dupliquer toutes les couches du premier
      réseau en n'ayant que sa couche cachée en entrée.
      
      L'apprentissage du second réseau, n'affecte pas les poids entre la couche d'entrée et la 
      couche cachée du premier réseau.
      
      Toutes les 1000 époques, l'objectif de la tâche est changé (par une autre combinaison).
      
      Nous avons déroulé l'expérience en 2 fois, une fois où on fige les poids
      entre les entrées et la couche cachée du premier réseau après les 1000 premières époques, 
      et une seconde où on fige les poids des couches cachées ( entrée - couche cachée ) des 2 réseaux.
    \paragraph{Schéma}
      \begin{center}
	\includegraphics[width=220px]{data/expA3/schema.png}
      \end{center}
      
    \paragraph{Paramètres}
      \begin{center}
	\begin{tabular}{lr}
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item momentum : 0.5 sur les 2 réseaux
	      \item taux d'apprentissage : 0.1 sur les 2 réseaux
	      \item \textbf{1600 formes} de chiffres différents présentées (shuffle) \cite{Handwritten_256}
	      \item apprentissage 10 (formes) x 4000 (époques)
	    \end{itemize}
	  \end{minipage}
	  &
	  \begin{minipage}{230px}
	    \begin{itemize}
	      \item poids initialisés sur [-0.25 ; 0.25]
	      \item taux d'apprentissage constant
	      \item entrées valent 0 ou 1
	      \item sigmoïde à température 1
	      \item utilisation de biais
	    \end{itemize}
	  \end{minipage}
	\end{tabular}
      \end{center}

  
  \newpage
  \subsection{Résultats}
    \paragraph{Principaux}
      Analyse des performances
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expB4/err_bb.png}
	  &
	  \includegraphics[width=250px]{data/expB4/err_block.png} \\
	  poids figés dans les 2 couches cachées
	  &
	  poids figés dans la couche cachée du FoN
	\end{tabular}
      \end{center}
      \subparagraph{Notes}
	\begin{itemize}
	  \item l'erreur de classification représente le taux de mauvaises réponses pour les 10 formes présentées sur une époque
	  \item pour SoN input layer et hidden layer, un winner-take-all n'est pas possible (ce n'est pas de la classification, 
	  mais une duplication), il y a donc un seuil d'erreur qui ne doit pas être dépassé par un seul neurone
	\end{itemize}
      \subparagraph{Conclusion}
	\begin{itemize}
	  \item lorsque l'on fige les poids, le premier réseau arrive à se réadapter aux nouvelles tâches
	  \item lorsqu'on fige également les poids du SoN, il est capable d'apprendre à dupliquer la sortie
	  du FoN
	  
	\end{itemize}
    \paragraph{Secondaires}
       Analyse des performances RMS
      \begin{center}
	\begin{tabular}{lr}
	  \hspace*{-1cm}
	  \includegraphics[width=250px]{data/expB4/rms_bb.png}
	  &
	  \includegraphics[width=250px]{data/expB4/rms_block.png} \\
	  poids figés dans les 2 couches cachées
	  &
	  poids figés dans la couche cachée du FoN
	\end{tabular}
      \end{center} 
      \subparagraph{Notes}
	\begin{itemize}
	  \item formule utilisée pour RMS (cf. Formules~\nameref{rms})
	  \item les courbes SoN layer représentent les erreurs (du second réseaux) sur les couches à reproduire 
	  \item la courbe RMS verte (SoN) est la somme des 3 courbes SoN layer
	\end{itemize}
      \subparagraph{Conclusion}
	Ces courbes sont a peu près équivalentes à celles de classifications.
	\\
	Même avec un RMS très faible pour la couche de sortie, dans la courbe de droite,
	cela suffit au réseau pour se tromper dans l'erreur de classification.

  \subsection{Conclusion}
  Preuve que le SoN subit le même phénomène (il faut bloquer des poids pour qu'il puisse s'adapter) que le FoN lors du changement de tâche.
  

  \newpage 
  \subsection{Formules}
    \input{rms}
    \input{gradient}

\bibliographystyle{../pre-rapport/apalike}
\bibliography{../pre-rapport/biblio}
