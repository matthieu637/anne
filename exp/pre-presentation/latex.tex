\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc} %commentaire
\usepackage[francais]{babel} %FR
\usepackage[T1]{fontenc} 

\usepackage[pdftex]{graphicx} % img
\usepackage{wrapfig}

\usepackage{algpseudocode}

\usepackage[a4paper]{geometry} %Réduire les marges

% Style Page
\pagestyle{headings} % entêtes avec titres des sections en haut de pagestyle

\sloppy % ne pas faire déborder les lignes dans la marge

\begin{document}

Formule RMS

$ rms\ proportion_{e} = \frac{ rms_{e} = \sqrt{ \frac{1}{n} \sum \limits_{i=1}^{n} ( o_{i,e} - d_{i} )^2 }}{max(rms_{e}),\ \forall e \in epochs } $

$ with \left\lbrace \begin{array}{lll} n : number\ of\ neurons\ on\ the\ output\ layer\\o_{i,e} : value\ obtained\ for\ the\ i^{th}\ neuron\ at\ the\ e^{th}\ epoch\\d_{i} : value\ desired \ for\ the\ i^{th}\ neuron\end{array} \right.$
         
$ rms\ proportion_{e} = \frac{ rms_{e} = \sqrt{ \frac{1}{n} \sum \limits_{i=1}^{n} ( o_{i,e} - d_{i} )^2 }}{max(rms_{e}),\ \forall e \in epochs } $

$ with \left\lbrace \begin{array}{lll} n : number\ of\ neurons\ on\ the\ output\ layer\\o_{i,e} : value\ obtained\ for\ the\ i^{th}\ neuron\ at\ the\ e^{th}\ epoch\\d_{i} : value\ desired \ for\ the\ i^{th}\ neuron\end{array} \right.$
         


\newpage


\begin{algorithmic}

\Function{discretize}{$hiddenNeuron[], piece$}
\State $result \gets 0$
\For{$i = 0 \to hiddenNeuron.length $} 
\State $result \gets result + piece^{i} \times cutting(hiddenNeuron[i], piece) $
\State $i \gets i + 1$
\EndFor
\State \Return result
\EndFunction

\end{algorithmic}

 -\\[4cm]

\begin{algorithmic}

\State $first\_order.calc\_hidden\_layer(samples.inputs)$
\State $high\_order.calc\_output\_layer(first\_order.hidden\_layer)$
\State $first\_order.calc\_output\_layer(first\_order.hidden\_layer,\ [0, ..., 0])$
\State 
\State $h\_output \gets ampli(high\_order.output\_layer)$
\State $right\_houtput \gets [0,\ 0]$
\If{$good\_answer(first\_order)$}
  \State $right\_houtput[1] \gets 1$
\Else
  \State $right\_houtput[0] \gets 1$
\EndIf
\State $first\_order.calc\_output\_layer(first\_order.hidden\_layer,\ h\_output)$
\State
\State $calc\_stats()$
\State
\State $high\_order.train(first\_order.hidden\_layer,\ right\_houtput)$
\State $first\_order.train(samples.inputs,\ samples.outputs,\ h\_output)$


\end{algorithmic}


\newpage

\begin{algorithmic}

\Function{$train$}{$inputs,\ outputs,\ add$}
\State $y \gets build\_error\_vector(...)$
\State $update\_weights\_hidden\_layer( ...)$
\\
\For {$i = 0 \to output\_neurons.length$ } 
\State $output\_neurons[i].update\_weights\_gradient(y[i],\ hidden\_neurons,\ add)$
\State $output\_neurons[i].update\_weights\_perceptron(outputs[i],\ hidden\_neurons,\ add)$
\EndFor
\EndFunction

\end{algorithmic}


\begin{algorithmic}

\Function{$update\_weights\_gradient$}{$error,\ intputs,\ add$}
\State $calc\_output(inputs + add)$
\\
\For {$j = 0 \to inputs.length$ } 
\State $dw \gets weights[j] - last\_weights[j]$
\State $p \gets error \times inputs[j]$
\State $weights[j] \gets weights[j] + learning\_rate \times p + momentum \times dw$
\EndFor
\EndFunction

\end{algorithmic}


\begin{algorithmic}

\Function{$update\_weights\_perceptron$}{$goal,\ intputs,\ add$}
\State $calc\_output(inputs + add)$
\\
\For {$j = inputs.length \to inputs.length + add.length$ } 
\State $dw \gets weights[j] - last\_weights[j]$
\State $p \gets (goal - state ) \times add[inputs.length - j]$
\State $weights[j] \gets weights[j] + \frac{learning\_rate \times p + momentum \times dw}{add.length} $
\EndFor
\EndFunction

\end{algorithmic}


\end{document}
